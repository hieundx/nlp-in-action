{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'harry',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'get',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = \"The faster Harry got to the store, the faster Harry, the faster, would get home.\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokens = tokenizer.tokenize(sentence.lower())\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 4,\n",
       "         'faster': 3,\n",
       "         'harry': 2,\n",
       "         'got': 1,\n",
       "         'to': 1,\n",
       "         'store': 1,\n",
       "         ',': 3,\n",
       "         'would': 1,\n",
       "         'get': 1,\n",
       "         'home': 1,\n",
       "         '.': 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "bag_of_words = Counter(tokens)\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4), ('faster', 3), (',', 3), ('harry', 2)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.most_common(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word \"harry\" appears 2 times in the sentence.\n",
      "The number of unique words in the sentence is 11.\n",
      "The term frequency of the word \"harry\" is 0.1818.\n"
     ]
    }
   ],
   "source": [
    "time_harry_appears = bag_of_words['harry']\n",
    "num_unique_words = len(bag_of_words)\n",
    "\n",
    "print('The word \"harry\" appears {} times in the sentence.'.format(time_harry_appears))\n",
    "print('The number of unique words in the sentence is {}.'.format(num_unique_words))\n",
    "\n",
    "tf = time_harry_appears / num_unique_words\n",
    "print('The term frequency of the word \"harry\" is {}.'.format(round(tf, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency of the term is way more important that the raw count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 26), ('a', 20), ('kite', 16), (',', 15), ('and', 10)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "from nlpia.data.loaders import kite_text\n",
    "tokens = tokenizer.tokenize(kite_text.lower())\n",
    "token_counts = Counter(tokens)\n",
    "token_counts.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are a lot of stop words. Let's discard them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kite', 16),\n",
       " (',', 15),\n",
       " ('kites', 8),\n",
       " ('wing', 5),\n",
       " ('lift', 4),\n",
       " ('may', 4),\n",
       " ('also', 3),\n",
       " ('kiting', 3),\n",
       " ('flown', 3),\n",
       " ('tethered', 2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "tokens = [x for x in tokens if x not in stopwords]\n",
    "kite_counts = Counter(tokens)\n",
    "kite_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at this BOW, we can tell that this document has something about kite flying. Things get a bit more interesting when there are more documents.\n",
    "\n",
    "# Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07207207207207207,\n",
       " 0.06756756756756757,\n",
       " 0.036036036036036036,\n",
       " 0.02252252252252252,\n",
       " 0.018018018018018018,\n",
       " 0.018018018018018018,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.013513513513513514,\n",
       " 0.009009009009009009]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vector = []\n",
    "doc_length = len(tokens)\n",
    "for key, value in kite_counts.most_common():\n",
    "    document_vector.append(value / doc_length)\n",
    "document_vector[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'faster', 'harry', 'got', 'to', 'the', 'store', ',', 'the', 'faster', 'and', 'faster', 'harry', 'would', 'get', 'home', '.']\n",
      "['harry', 'is', 'hairy', 'and', 'faster', 'than', 'jill', '.']\n",
      "['jill', 'is', 'not', 'as', 'hairy', 'as', 'harry', '.']\n",
      "doc0: 17\n",
      "doc1: 8\n",
      "doc2: 8\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    \"The faster Harry got to the store, the faster and faster Harry would get home.\",\n",
    "    \"Harry is hairy and faster than Jill.\",\n",
    "    \"Jill is not as hairy as Harry.\"\n",
    "]\n",
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    print(tokens)\n",
    "    doc_tokens += [sorted(tokens)]\n",
    "\n",
    "for i, doc in enumerate(doc_tokens):\n",
    "    print(\"doc{}: {}\".format(i, len(doc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "len(all_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "[',', '.', 'and', 'as', 'faster', 'get', 'got', 'hairy', 'harry', 'home', 'is', 'jill', 'not', 'store', 'than', 'the', 'to', 'would']\n"
     ]
    }
   ],
   "source": [
    "lexicon = sorted(set(all_doc_tokens))\n",
    "print(len(lexicon))\n",
    "print(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([(',', 0),\n",
       "             ('.', 0),\n",
       "             ('and', 0),\n",
       "             ('as', 0),\n",
       "             ('faster', 0),\n",
       "             ('get', 0),\n",
       "             ('got', 0),\n",
       "             ('hairy', 0),\n",
       "             ('harry', 0),\n",
       "             ('home', 0),\n",
       "             ('is', 0),\n",
       "             ('jill', 0),\n",
       "             ('not', 0),\n",
       "             ('store', 0),\n",
       "             ('than', 0),\n",
       "             ('the', 0),\n",
       "             ('to', 0),\n",
       "             ('would', 0)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "zero_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([(',', 0.05555555555555555),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.16666666666666666),\n",
       "              ('get', 0.05555555555555555),\n",
       "              ('got', 0.05555555555555555),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.1111111111111111),\n",
       "              ('home', 0.05555555555555555),\n",
       "              ('is', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.05555555555555555),\n",
       "              ('than', 0),\n",
       "              ('the', 0.16666666666666666),\n",
       "              ('to', 0.05555555555555555),\n",
       "              ('would', 0.05555555555555555)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0.05555555555555555),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.05555555555555555),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('than', 0.05555555555555555),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.05555555555555555),\n",
       "              ('and', 0),\n",
       "              ('as', 0.1111111111111111),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.05555555555555555),\n",
       "              ('harry', 0.05555555555555555),\n",
       "              ('home', 0),\n",
       "              ('is', 0.05555555555555555),\n",
       "              ('jill', 0.05555555555555555),\n",
       "              ('not', 0.05555555555555555),\n",
       "              ('store', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    \n",
    "    for key, value in token_counts.items():\n",
    "        vec[key] = value / len(lexicon)\n",
    "    doc_vectors.append(vec)\n",
    "doc_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def consine_sim(vec1, vec2):\n",
    "    vec1 = [val for val in vec1.values()]\n",
    "    vec2 = [val for val in vec2.values()]\n",
    "    dot_prod = 0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "    return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zipf's law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\lived\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "brown.words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 69971),\n",
       " ('of', 36412),\n",
       " ('and', 28853),\n",
       " ('to', 26158),\n",
       " ('a', 23195),\n",
       " ('in', 21337),\n",
       " ('that', 10594),\n",
       " ('is', 10109),\n",
       " ('was', 9815),\n",
       " ('he', 9548)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "puncs = set((',', '.', '--', '-', '!', '?', ':', ';', '``', \"''\", '(', ')', '[', ']'))\n",
    "word_list = [word.lower() for word in brown.words() if word not in puncs]\n",
    "word_counts = Counter(word_list)\n",
    "word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common word (\"the\") is **twice** as frequent as the next common words (\"of\", \"and\"), **three times** as common as the next one (\"a\", \"in\"), and **six times** as common as \"that\", \"is\", \"was\", and \"he\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "When there are multiple documents, words with low frequency against the lexicon can be important in the document where it belongs to. To counter this, we need **inverse document frequency**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in intro: 363\n",
      "Number of tokens in history: 297\n"
     ]
    }
   ],
   "source": [
    "from nlpia.data.loaders import kite_text, kite_history\n",
    "kite_intro = kite_text.lower()\n",
    "intro_tokens = tokenizer.tokenize(kite_intro)\n",
    "\n",
    "kite_history = kite_history.lower()\n",
    "history_tokens = tokenizer.tokenize(kite_history)\n",
    "\n",
    "intro_total = len(intro_tokens)\n",
    "print('Number of tokens in intro:', intro_total)\n",
    "\n",
    "history_total = len(history_tokens)\n",
    "print('Number of tokens in history:', history_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of \"kite\" in intro is: 0.0441\n",
      "Term Frequency of \"kite\" in history is: 0.0202\n"
     ]
    }
   ],
   "source": [
    "intro_tf = {}\n",
    "history_tf = {}\n",
    "\n",
    "intro_counts  = Counter(intro_tokens)\n",
    "history_counts = Counter(history_tokens)\n",
    "\n",
    "intro_tf['kite'] = intro_counts['kite'] / intro_total\n",
    "history_tf['kite'] = history_counts['kite'] / history_total\n",
    "\n",
    "print('Term Frequency of \"kite\" in intro is:', round(intro_tf['kite'], 4))\n",
    "print('Term Frequency of \"kite\" in history is:', round(history_tf['kite'], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word \"kite\" appears more often in the intro than the history. But the question is, is it really more important in the intro?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term Frequency of \"and\" in intro is: 0.0275\n",
      "Term Frequency of \"and\" in history is: 0.0303\n"
     ]
    }
   ],
   "source": [
    "intro_tf['and'] = intro_counts['and'] / intro_total\n",
    "history_tf['and'] = history_counts['and'] / history_total\n",
    "\n",
    "print('Term Frequency of \"and\" in intro is:', round(intro_tf['and'], 4))\n",
    "print('Term Frequency of \"and\" in history is:', round(history_tf['and'], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to think about IDF is this: if a term appears a lot in a document, but rarely in others, it is probably important for that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF for \"and\" in intro is: 0.0275\n",
      "TF-IDF for \"and\" in history is: 0.0303\n",
      "\n",
      "TF-IDF for \"kite\" in intro is: 0.0441\n",
      "TF-IDF for \"kite\" in history is: 0.0202\n",
      "\n",
      "TF-IDF for \"china\" in intro is: 0.0\n",
      "TF-IDF for \"china\" in history is: 0.0202\n"
     ]
    }
   ],
   "source": [
    "num_docs_containing_and = 0\n",
    "for doc in [intro_tokens, history_tokens]:\n",
    "    if 'and' in doc:\n",
    "        num_docs_containing_and += 1\n",
    "        \n",
    "intro_tf['china'] = intro_counts['china'] / intro_total\n",
    "history_tf['china'] = history_counts['china'] / history_total\n",
    "\n",
    "num_docs = len([intro_tokens, history_tokens])\n",
    "intro_idf = {}\n",
    "history_idf = {}\n",
    "\n",
    "intro_idf['and'] = num_docs / num_docs_containing_and\n",
    "history_idf['and'] = num_docs / num_docs_containing_and\n",
    "\n",
    "\n",
    "intro_idf['kite'] = num_docs / 2\n",
    "history_idf['kite'] = num_docs / 2\n",
    "\n",
    "intro_idf['china'] = num_docs / 1\n",
    "history_idf['china'] = num_docs / 1\n",
    "\n",
    "intro_tfidf = {}\n",
    "intro_tfidf['and'] = intro_tf['and'] * intro_idf['and']\n",
    "intro_tfidf['kite'] = intro_tf['kite'] * intro_idf['kite']\n",
    "intro_tfidf['china'] = intro_tf['china'] * intro_idf['china']\n",
    "\n",
    "history_tfidf = {}\n",
    "history_tfidf['and'] = history_tf['and'] * history_idf['and']\n",
    "history_tfidf['kite'] = history_tf['kite'] * history_idf['kite']\n",
    "history_tfidf['china'] = history_tf['china'] * history_idf['china']\n",
    "\n",
    "print('TF-IDF for \"and\" in intro is:', round(intro_tfidf['and'], 4))\n",
    "print('TF-IDF for \"and\" in history is:', round(history_tfidf['and'], 4))\n",
    "print()\n",
    "\n",
    "print('TF-IDF for \"kite\" in intro is:', round(intro_tfidf['kite'], 4))\n",
    "print('TF-IDF for \"kite\" in history is:', round(history_tfidf['kite'], 4))\n",
    "print()\n",
    "\n",
    "print('TF-IDF for \"china\" in intro is:', round(intro_tfidf['china'], 4))\n",
    "print('TF-IDF for \"china\" in history is:', round(history_tfidf['china'], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tfidf_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    \n",
    "    for term, count in token_counts.items():\n",
    "        docs_containing_term = 0\n",
    "        for _doc in docs:\n",
    "            if term in _doc:\n",
    "                docs_containing_term += 1\n",
    "        tf = count / len(lexicon)\n",
    "        if docs_containing_term:\n",
    "            idf = len(docs) / docs_containing_term\n",
    "        else:\n",
    "            idf = 0\n",
    "        vec[term] = tf * idf\n",
    "    document_tfidf_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(',', 0.16666666666666666), ('.', 0.05555555555555555), ('and', 0.08333333333333333), ('as', 0), ('faster', 0.25), ('get', 0.16666666666666666), ('got', 0.16666666666666666), ('hairy', 0), ('harry', 0.0), ('home', 0.16666666666666666), ('is', 0), ('jill', 0), ('not', 0), ('store', 0.16666666666666666), ('than', 0), ('the', 0.5), ('to', 0.16666666666666666), ('would', 0.16666666666666666)])\n",
      "OrderedDict([(',', 0), ('.', 0.05555555555555555), ('and', 0.08333333333333333), ('as', 0), ('faster', 0.08333333333333333), ('get', 0), ('got', 0), ('hairy', 0.08333333333333333), ('harry', 0.0), ('home', 0), ('is', 0.08333333333333333), ('jill', 0.0), ('not', 0), ('store', 0), ('than', 0.16666666666666666), ('the', 0), ('to', 0), ('would', 0)])\n",
      "OrderedDict([(',', 0), ('.', 0.05555555555555555), ('and', 0), ('as', 0.1111111111111111), ('faster', 0), ('get', 0), ('got', 0), ('hairy', 0.08333333333333333), ('harry', 0.0), ('home', 0), ('is', 0.08333333333333333), ('jill', 0.0), ('not', 0.16666666666666666), ('store', 0), ('than', 0), ('the', 0), ('to', 0), ('would', 0)])\n"
     ]
    }
   ],
   "source": [
    "for vec in document_tfidf_vectors:\n",
    "    print(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having built the TF-IDF vectors of the documents, we now use cosine similarity to find similar documents given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([(',', 0), ('.', 0), ('and', 0), ('as', 0), ('faster', 0), ('get', 0.2727272727272727), ('got', 0), ('hairy', 0), ('harry', 0), ('home', 0), ('is', 0), ('jill', 0), ('not', 0), ('store', 0.2727272727272727), ('than', 0), ('the', 0.2727272727272727), ('to', 0.5454545454545454), ('would', 0)])\n"
     ]
    }
   ],
   "source": [
    "query = \"How long does it take to get to the store?\"\n",
    "query_vec = copy.copy(zero_vector)\n",
    "\n",
    "tokens = tokenizer.tokenize(query.lower())\n",
    "token_counts = Counter(tokens)\n",
    "\n",
    "for term, count in token_counts.items():\n",
    "    docs_containing_term = 0\n",
    "    for _doc in docs:\n",
    "        if term in _doc:\n",
    "            docs_containing_term += 1\n",
    "    if docs_containing_term == 0:\n",
    "        continue\n",
    "    tf = count / len(tokens)\n",
    "    idf = len(docs) / docs_containing_term\n",
    "    query_vec[term] = tf * idf\n",
    "print(query_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6132857433407973\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for vec in document_tfidf_vectors:\n",
    "    print(consine_sim(query_vec, vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can safely say that the first document is most similar to our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  How long does it take to get to the store?\n",
      "Most similar document:  The faster Harry got to the store, the faster and faster Harry would get home.\n"
     ]
    }
   ],
   "source": [
    "print('Query: ', query)\n",
    "print('Most similar document: ', docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "We can use `scikit-learn` to implement the above model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16 0.   0.48 0.21 0.21 0.   0.25 0.21 0.   0.   0.   0.21 0.   0.64\n",
      "  0.21 0.21]\n",
      " [0.37 0.   0.37 0.   0.   0.37 0.29 0.   0.37 0.37 0.   0.   0.49 0.\n",
      "  0.   0.  ]\n",
      " [0.   0.75 0.   0.   0.   0.29 0.22 0.   0.29 0.29 0.38 0.   0.   0.\n",
      "  0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = docs\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "model = vectorizer.fit_transform(corpus)\n",
    "tfidf_vecs = model.todense().round(2)\n",
    "print(tfidf_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.38, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.38, 0.  , 0.38, 0.76, 0.  ]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vec_tfidf = vectorizer.transform([query])\n",
    "query_vec_tfidf.todense().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56144043, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(query_vec_tfidf, tfidf_vecs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpia1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
