{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-gram\n",
    "First we load the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "url = ('https://gitlab.com/tangibleai/nlpia2/-/raw/main/src/nlpia2/ch03/bias_intro.txt')\n",
    "response = requests.get(url)\n",
    "\n",
    "bias_intro = response.text\n",
    "docs = [nlp(s) for s in bias_intro.split('\\n') if s.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithmic bias describes systematic and repeatable errors in a computer system that create unfair outcomes, such as privileging one arbitrary group of users over others.\n",
      "Bias can emerge due to many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm.\n",
      "Algorithmic bias is found across platforms, including but not limited to search engine results and social media platforms, and can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity.\n",
      "The study of algorithmic bias is most concerned with algorithms that reflect \"systematic and unfair\" discrimination.\n",
      "This bias has only recently been addressed in legal frameworks, such as the 2018 European Union's General Data Protection Regulation.\n",
      "More comprehensive regulation is needed as emerging technologies become increasingly advanced and opaque.\n",
      "As algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world.\n",
      "Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise, and in some cases, reliance on algorithms can displace human responsibility for their outcomes.\n",
      "Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.\n",
      "Algorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech.\n",
      "It has also arisen in criminal justice, healthcare, and hiring, compounding existing racial, economic, and gender biases.\n",
      "The relative inability of facial recognition technology to accurately identify darker-skinned faces has been linked to multiple wrongful arrests of men of color, an issue stemming from imbalanced datasets.\n",
      "Problems in understanding, researching, and discovering algorithmic bias persist due to the proprietary nature of algorithms, which are typically treated as trade secrets.\n",
      "Even when full transparency is provided, the complexity of certain algorithms poses a barrier to understanding their functioning.\n",
      "Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis.\n",
      "In many cases, even within a single website or application, there is no single \"algorithm\" to examine, but a network of many interrelated programs and data inputs, even between users of the same service.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [doc.text for doc in docs]\n",
    "for doc in docs:\n",
    "    print(doc)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "count_vectors = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the document we want to search for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = 'What is algorithmic bias?'\n",
    "question_vec = vectorizer.transform([question])\n",
    "question_vec.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform cosine similarity against the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23570226]\n",
      " [0.12451456]\n",
      " [0.24743583]\n",
      " [0.4330127 ]\n",
      " [0.12909944]\n",
      " [0.16012815]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.1490712 ]\n",
      " [0.27216553]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.24077171]\n",
      " [0.14002801]\n",
      " [0.        ]\n",
      " [0.09128709]]\n",
      "Question:  What is algorithmic bias?\n",
      "Match:  The study of algorithmic bias is most concerned with algorithms that reflect \"systematic and unfair\" discrimination.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similar_docs = cosine_similarity(count_vectors, question_vec)\n",
    "print(similar_docs)\n",
    "\n",
    "most_similar_doc_index = similar_docs.argmax()\n",
    "most_similar_doc = docs[most_similar_doc_index]\n",
    "\n",
    "print('Question: ', question)\n",
    "print('Match: ', most_similar_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad result, but the first sentence of the corpus is probably a better match. Let's see how n-grams can help improve the precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<16x616 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 772 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "ngram_vectors = ngram_vectorizer.fit_transform(corpus)\n",
    "ngram_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is algorithmic bias?\n",
      "Match:  The study of algorithmic bias is most concerned with algorithms that reflect \"systematic and unfair\" discrimination.\n"
     ]
    }
   ],
   "source": [
    "ngram_question_vec = ngram_vectorizer.transform([question])\n",
    "\n",
    "similar_docs = cosine_similarity(ngram_vectors, ngram_question_vec)\n",
    "\n",
    "most_similar_doc_index = similar_docs.argmax()\n",
    "most_similar_doc = docs[most_similar_doc_index]\n",
    "\n",
    "print('Question: ', question)\n",
    "print('Match: ', most_similar_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
