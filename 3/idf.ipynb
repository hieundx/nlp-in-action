{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://gitlab.com/tangibleai/nlpia2/-/raw/main/src/nlpia2/ch03/bias_discrimination.txt'\n",
    "\n",
    "bias_discrimination = requests.get(url).text\n",
    "\n",
    "url = 'https://gitlab.com/tangibleai/nlpia2/-/raw/main/src/nlpia2/ch03/bias_intro.txt'\n",
    "response = requests.get(url)\n",
    "\n",
    "bias_intro = requests.get(url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intro tokens: 498\n",
      "Disc tokens: 454\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "intro_tokens = [token.text for token in nlp(bias_intro)]\n",
    "disc_tokens = [token.text for token in nlp(bias_discrimination)]\n",
    "\n",
    "intro_total = len(intro_tokens)\n",
    "disc_total = len(disc_tokens)\n",
    "\n",
    "print(f'Intro tokens: {intro_total}')\n",
    "print(f'Disc tokens: {disc_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF of \"bias\" in intro: 0.0120\n",
      "TF of \"bias\" in disc: 0.0022\n",
      "\"bias\" appears about 5.47 times more in the intro than in the text\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "intro_tf = {}\n",
    "disc_tf = {}\n",
    "intro_counts = Counter(intro_tokens)\n",
    "intro_tf['bias'] = intro_counts['bias'] / intro_total\n",
    "disc_counts = Counter(disc_tokens)\n",
    "disc_tf['bias'] = disc_counts['bias'] / disc_total\n",
    "\n",
    "print('TF of \"bias\" in intro: {:.4f}'.format(intro_tf['bias']))\n",
    "print('TF of \"bias\" in disc: {:.4f}'.format(disc_tf['bias']))\n",
    "print('\"bias\" appears about {:.2f} times more in the intro than in the text'.format(intro_tf['bias'] / disc_tf['bias']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term \"bias\" appears more in intro than in discrimination text. Does it mean the intro is more about \"bias\" than the text? Not really. Because if we examine the TF of the term \"and\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF of \"and\" in intro: 0.0281\n",
      "TF of \"and\" in disc: 0.0110\n",
      "\"and\" appears about 2.55 times more in the intro than in the text\n"
     ]
    }
   ],
   "source": [
    "intro_tf['and'] = intro_counts['and'] / intro_total\n",
    "disc_tf['and'] = disc_counts['and'] / disc_total\n",
    "\n",
    "print('TF of \"and\" in intro: {:.4f}'.format(intro_tf['and']))\n",
    "print('TF of \"and\" in disc: {:.4f}'.format(disc_tf['and']))\n",
    "print('\"and\" appears about {:.2f} times more in the intro than in the text'.format(intro_tf['and'] / disc_tf['and']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good way to think of a term’s inverse document frequency is this: How surprising is it that this token is in this document? The concept of measuring the surprise in a token might not sound like a very mathematical idea. However, in statistics, physics and information theory, the surprise of a symbol is used to measure its entropy or information content. And that is exactly what you need to gage the importance of a particular word. If a term appears in one document a lot of times, but occurs rarely in the rest of the corpus, it is a word that distinguishes that document’s meaning from the other documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intro IDF:\n",
      "and: 1.0000\n",
      "bias: 1.0000\n",
      "black: 2.0000\n"
     ]
    }
   ],
   "source": [
    "intro_idf = {}\n",
    "num_docs_containing_term = {}\n",
    "num_docs = len([intro_tokens, disc_tokens])\n",
    "\n",
    "terms = [\"and\", \"bias\", \"black\"]\n",
    "\n",
    "for term in terms:\n",
    "    for doc in [intro_tokens, disc_tokens]:\n",
    "        if term in doc:\n",
    "            if term in num_docs_containing_term:\n",
    "                num_docs_containing_term[term] += 1\n",
    "            else:\n",
    "                num_docs_containing_term[term] = 1\n",
    "\n",
    "for term in terms:\n",
    "    intro_idf[term] = num_docs / num_docs_containing_term[term]\n",
    "\n",
    "print(\"Intro IDF:\")\n",
    "for term, idf in intro_idf.items():\n",
    "    print(f\"{term}: {idf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance ranking\n",
    "First we build a vector of TF-IDF for all documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OrderedDict([(',', 0.18181818181818182),\n",
       "              ('.', 0.045454545454545456),\n",
       "              ('1', 0),\n",
       "              ('and', 0.09090909090909091),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.2727272727272727),\n",
       "              ('get', 0.09090909090909091),\n",
       "              ('got', 0.18181818181818182),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0.18181818181818182),\n",
       "              ('hour', 0),\n",
       "              ('is', 0),\n",
       "              ('it', 0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.09090909090909091),\n",
       "              ('takes', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0.2727272727272727),\n",
       "              ('to', 0.09090909090909091),\n",
       "              ('would', 0.18181818181818182)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.045454545454545456),\n",
       "              ('1', 0),\n",
       "              ('and', 0.09090909090909091),\n",
       "              ('as', 0),\n",
       "              ('faster', 0.09090909090909091),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.09090909090909091),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('hour', 0),\n",
       "              ('is', 0.09090909090909091),\n",
       "              ('it', 0),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0),\n",
       "              ('store', 0),\n",
       "              ('takes', 0),\n",
       "              ('than', 0.18181818181818182),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.045454545454545456),\n",
       "              ('1', 0),\n",
       "              ('and', 0),\n",
       "              ('as', 0.12121212121212122),\n",
       "              ('faster', 0),\n",
       "              ('get', 0),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0.09090909090909091),\n",
       "              ('harry', 0.0),\n",
       "              ('home', 0),\n",
       "              ('hour', 0),\n",
       "              ('is', 0.09090909090909091),\n",
       "              ('it', 0),\n",
       "              ('jill', 0.0),\n",
       "              ('not', 0.18181818181818182),\n",
       "              ('store', 0),\n",
       "              ('takes', 0),\n",
       "              ('than', 0),\n",
       "              ('the', 0),\n",
       "              ('to', 0),\n",
       "              ('would', 0)]),\n",
       " OrderedDict([(',', 0),\n",
       "              ('.', 0.045454545454545456),\n",
       "              ('1', 0.18181818181818182),\n",
       "              ('and', 0),\n",
       "              ('as', 0),\n",
       "              ('faster', 0),\n",
       "              ('get', 0.09090909090909091),\n",
       "              ('got', 0),\n",
       "              ('hairy', 0),\n",
       "              ('harry', 0),\n",
       "              ('home', 0),\n",
       "              ('hour', 0.18181818181818182),\n",
       "              ('is', 0),\n",
       "              ('it', 0.0),\n",
       "              ('jill', 0),\n",
       "              ('not', 0),\n",
       "              ('store', 0.09090909090909091),\n",
       "              ('takes', 0.18181818181818182),\n",
       "              ('than', 0),\n",
       "              ('the', 0.09090909090909091),\n",
       "              ('to', 0.18181818181818182),\n",
       "              ('would', 0)])]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "docs = [\n",
    "    \"The faster Harry got to the store, the faster and faster Harry would get home.\",\n",
    "    \"Harry is hairy and faster than Jill.\",\n",
    "    \"Jill is not as hairy as Harry.\",\n",
    "    \"It takes 1 hour to get to the store.\",\n",
    "]\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "\n",
    "zero_vector = copy.copy(intro_idf)\n",
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "lexicon = sorted(set(all_doc_tokens))\n",
    "\n",
    "from collections import OrderedDict\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "\n",
    "document_tfidf_vectors = []\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    \n",
    "    for key, value in token_counts.items():\n",
    "        docs_containing_key = 0\n",
    "        for _doc in docs:\n",
    "            if key in _doc:\n",
    "                docs_containing_key += 1\n",
    "        tf = value / len(lexicon)\n",
    "        if docs_containing_key:\n",
    "            idf = len(docs) / docs_containing_key\n",
    "        else:\n",
    "            idf = 0\n",
    "        vec[key] = tf * idf\n",
    "    document_tfidf_vectors.append(vec)\n",
    "document_tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query vector: OrderedDict([(',', 0), ('.', 0), ('1', 0), ('and', 0), ('as', 0), ('faster', 0), ('get', 0.18181818181818182), ('got', 0), ('hairy', 0), ('harry', 0), ('home', 0), ('hour', 0), ('is', 0), ('it', 0.36363636363636365), ('jill', 0), ('not', 0), ('store', 0.18181818181818182), ('takes', 0), ('than', 0), ('the', 0.18181818181818182), ('to', 0.36363636363636365), ('would', 0), ('take', 0.36363636363636365)])\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "\n",
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "lexicon = sorted(set(all_doc_tokens))\n",
    "\n",
    "from collections import OrderedDict\n",
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "\n",
    "query = \"How long does it take to get to the store?\"\n",
    "query_vec = copy.copy(zero_vector)\n",
    "\n",
    "tokens = [token.text for token in nlp(query.lower())]\n",
    "token_counts = Counter(tokens)\n",
    "\n",
    "for token, count in token_counts.items():\n",
    "    docs_containing_token = 0\n",
    "    for _doc in docs:\n",
    "        if token in _doc.lower():\n",
    "            docs_containing_token += 1\n",
    "    if docs_containing_token == 0:\n",
    "        continue\n",
    "    \n",
    "    tf = count / len(tokens)\n",
    "    idf = len(docs) / docs_containing_token\n",
    "    query_vec[token] = tf * idf\n",
    "    \n",
    "print(\"Query vector:\", query_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cosine_sim(vec1, vec2):\n",
    "    vec1 = [val for val in vec1.values()]\n",
    "    vec2 = [val for val in vec2.values()]\n",
    "    \n",
    "    dot_prod =0\n",
    "    for i, v in enumerate(vec1):\n",
    "        dot_prod += v * vec2[i]\n",
    "    \n",
    "    mag_1 = math.sqrt(sum([x**2 for x in vec1]))\n",
    "    mag_2 = math.sqrt(sum([x**2 for x in vec2]))\n",
    "    \n",
    "    return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29223800250640314, 0.0, 0.0, 0.41194292043554986]\n",
      "Query: How long does it take to get to the store?\n",
      "Match: It takes 1 hour to get to the store.\n"
     ]
    }
   ],
   "source": [
    "cosine_sims = [cosine_sim(doc_vec, query_vec) for doc_vec in document_tfidf_vectors]\n",
    "print(cosine_sims)\n",
    "\n",
    "highest_score = max(cosine_sims)\n",
    "highest_score_index = cosine_sims.index(highest_score)\n",
    "print(\"Query:\", query)\n",
    "print(\"Match:\", docs[highest_score_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A faster way to tokenizer documents for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.18, 0.  , 0.55, 0.18, 0.23, 0.  , 0.3 , 0.23, 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.18, 0.  , 0.  , 0.55, 0.18, 0.23],\n",
       "       [0.37, 0.  , 0.37, 0.  , 0.  , 0.37, 0.3 , 0.  , 0.  , 0.37, 0.  ,\n",
       "        0.37, 0.  , 0.  , 0.  , 0.47, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.74, 0.  , 0.  , 0.  , 0.29, 0.24, 0.  , 0.  , 0.29, 0.  ,\n",
       "        0.29, 0.37, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.29, 0.  , 0.  , 0.  , 0.  , 0.37, 0.  , 0.37,\n",
       "        0.  , 0.  , 0.29, 0.37, 0.  , 0.29, 0.58, 0.  ]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = docs # type: ignore\n",
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "vectorizer = vectorizer.fit(corpus)\n",
    "vectors = vectorizer.transform(corpus)\n",
    "\n",
    "vectors.todense().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.34, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.43,\n",
       "        0.  , 0.  , 0.34, 0.  , 0.  , 0.34, 0.68, 0.  ]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vec = vectorizer.transform([query])\n",
    "query_vec.todense().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.43964217],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.85319029]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_sims = cosine_similarity(vectors, query_vec)\n",
    "cosine_sims"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
